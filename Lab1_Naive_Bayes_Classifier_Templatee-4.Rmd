---
editor_options:
  markdown:
    wrap: 72
---

```
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

## Work breakdown

-   *Gordiy Eliseev*: Data pre-processing, Data visualization
-   *Mykyta Yagoda*: Classifier implementation
-   *Vladyslav Shymanovskyi*: Measurements of effectiveness of your classifier, Conclusions

## Introduction

During the first three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the **Bayes
theorem**.

**Naive Bayes Classifier** is a simple algorithm, which is based on
**Bayes theorem** and used for solving classification problems.
**Classification problem** is a problem in which an observation has to
be classified in one of the $n$ classes based on its similarity with
observations in each class.

It is a **probabilistic classifier**, which means it predicts based on
the probability of an observation belonging to each class. To compute
it, this algorithm uses **Bayes' formula,** that you probably already
came across in **Lesson 3:**
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong **independence** assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given observation
(*For example, if an observation is presented as a sentence, then each
word can be a feature*). Thus,
$\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now can be calculated
as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated as corresponding
relative frequencies using available data\

**See [*this
link*](https://www.javatpoint.com/machine-learning-naive-bayes-classifier)
for more detailed explanations & examples :) Also you can watch [*this
video*](https://youtu.be/O2L2Uv9pdDA?si=-ohkHVDuu3sLLGMq) for more
examples!**

## Data description

There are 5 datasets uploaded on the cms (data.zip)

To determine your variant, take your team number from the list of teams
on cms and take *mod 5* - this is the number of your data set.

-   **0 - authors** This data set consists of citations of three famous
    writers: Edgar Alan Poe, Mary Wollstonecraft Shelley and HP
    Lovecraft. The task with this data set is to classify a piece of
    text with the author who was more likely to write it.

-   **1 - discrimination** This data set consists of tweets that have
    discriminatory (sexism or racism) messages or of tweets that are of
    neutral mood. The task is to determine whether a given tweet has
    discriminatory mood or does not.

-   **2 - fake news** This data set contains data of American news: a
    headline and an abstract of the article. Each piece of news is
    classified as fake or credible. The task is to classify the news
    from test.csv as credible or fake.

-   **3 - sentiment** All the text messages contained in this data set
    are labeled with three sentiments: positive, neutral or negative.
    The task is to classify some text message as the one of positive
    mood, negative or neutral.

-   **4 - spam** This last data set contains SMS messages classified as
    spam or non-spam (ham in the data set). The task is to determine
    whether a given message is spam or non-spam.

Each data set consists of two files: *train.csv* and *test.csv*. The
first one is used to find the probabilities of the corresponding classes
and the second one is used to test your classifier afterwards. Note that
it is crucial to randomly split your data into training and testing
parts to test the classifierʼs possibilities on the unseen data.

```{r}
# here goes a list of recommended libraries,
# though you may install other ones if they are needed

options(repos = c(CRAN = "https://cloud.r-project.org"))

if (!require("pacman")) install.packages("pacman")

pacman::p_load(
  dplyr,
  ggplot2,
  tidytext,
  readr,
  patchwork,
  wordcloud
)
```

## Outline of the work

1.  **Data pre-processing** (includes removing punctuation marks and
    stop words, representing each message as a bag-of-words)
2.  **Data visualization** (it's time to plot your data!)
3.  **Classifier implementation** (using the training set, calculate all
    the conditional probabilities in formula (1))
4.  **Measurements of effectiveness of your classifier** (use the
    results from the previous step to predict classes for messages in
    the testing set and measure the accuracy, precision and recall, F1
    score metric etc)
5.  **Conclusions**

*!! do not forget to submit both the (compiled) Rmd source file and the
.html output !!*

## Data pre-processing

-   Read the *.csv* data files.
-   Сlear your data from punctuation or other unneeded symbols.
-   Clear you data from stop words. You don't want words as is, and, or
    etc. to affect your probabilities distributions, so it is a wise
    decision to get rid of them. Find list of stop words in the cms
    under the lab task.
-   Represent each test message as its bag-of-words. Here:
    <https://machinelearningmastery.com/gentle-introduction-bag-words-model/>
    you can find general introduction to the bag-of-words model and
    examples on to create it.
-   It is highly recommended to get familiar with R dataframes, it would
    make the work much easier to do.
-   Useful links:
    -   <https://steviep42.github.io/webscraping/book/bagofwords.html#tidytext> -
        example of using *tidytext* to count frequencies of the words.
    -   Basics of Text Mining in R:
        <http://rstudio-pubs-static.s3.amazonaws.com/256588_57b585da6c054349825cba46685d8464.html>
        . Note that it also includes an example on how to create a bag
        of words from your text document.

```{r}
list.files(getwd())
list.files("data/0-authors")
```

```{r}
test_path <- "data/0-authors/test.csv"
train_path <- "data/0-authors/train.csv"

stop_words <- read_file("stop_words.txt")
splitted_stop_words <- strsplit(stop_words, "\n") %>%
  unlist() %>%
  gsub("\r", "", .) %>%
  unique()
stop_words
splitted_stop_words
```

```{r}
train <-  read.csv(file = train_path, stringsAsFactors = FALSE, row.names = 1)
test <-  read.csv(file = test_path, stringsAsFactors = FALSE, row.names = 1)
```

```{r}
# note the power functional features of R bring us!
tidy_text_train <- unnest_tokens(train, 'splitted', 'text', token="words") %>%
             filter(!splitted %in% splitted_stop_words)

tidy_text_test <- unnest_tokens(test, 'splitted', 'text', token="words") %>%
             filter(!splitted %in% splitted_stop_words)

t_counts <- tidy_text_train %>% count(splitted,sort=TRUE)
tidy_text_train
```

## Data visualization

Each time you work with some data, you need to understand it before you
start processing it. R has very powerful tools to make nice plots and
visualization. Show what are the most common words for negative and
positive examples as a histogram, word cloud etc. Be creative!

```{r}
t_counts_train <- tidy_text_train %>% count(splitted, sort = TRUE, name = "freq")
t_counts_test  <- tidy_text_test  %>% count(splitted, sort = TRUE, name = "freq")

# Top-20 barplot (train)
ggplot(t_counts_train %>% slice_max(freq, n = 20),
       aes(x = reorder(splitted, freq), y = freq)) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 20 words — TRAIN", x = "Word", y = "Frequency")

# Top-20 barplot (test)
ggplot(t_counts_test %>% slice_max(freq, n = 20),
       aes(x = reorder(splitted, freq), y = freq)) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 20 words — TEST", x = "Word", y = "Frequency")

# ----------  WORDCLOUDS ----------
set.seed(123)  # for reproducibility

# Wordcloud (train)
wordcloud(words = t_counts_train$splitted,
          freq  = t_counts_train$freq,
          min.freq = 20,
          max.words = 150,
          random.order = FALSE,
          colors = brewer.pal(8, "Dark2"))

# Wordcloud (test)
wordcloud(words = t_counts_test$splitted,
          freq  = t_counts_test$freq,
          min.freq = 20,
          max.words = 150,
          random.order = FALSE,
          colors = brewer.pal(8, "Dark2"))

# ---------- COMPARISON: TRAIN vs TEST (faceted barplots) ----------
both_counts <- bind_rows(
  t_counts_train %>% mutate(dataset = "train"),
  t_counts_test  %>% mutate(dataset = "test")
)

ggplot(both_counts %>% group_by(dataset) %>% slice_max(freq, n = 15),
       aes(x = reorder_within(splitted, freq, dataset), y = freq, fill = dataset)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~dataset, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 15 words — TRAIN vs TEST",
       x = "Word", y = "Frequency")
```

## Classifier implementation

```{r}
naiveBayes <- setRefClass(
  "naiveBayes",

  fields = list(
    classes = "character",     # class labels
    vocab   = "character",     # all words in the bag-of-words
    log_pr  = "numeric",       # log P(c)
    log_lik = "matrix"         # log P(w|c)
  ),

  methods = list(
    fit = function(X, y) {
      # X: document-term matrix (rows = docs, cols = words, counts as values)
      # y: vector of class labels for each document

      classes <<- unique(y)
      vocab   <<- colnames(X)
      V <- length(vocab)

      # Priors: P(c) = fraction of docs in each class
      pr <- table(y) / length(y)
      log_pr <<- as.vector(log(pr[classes])) #changed
      names(log_pr) <<- classes

      # Word counts per class
      class_counts <- sapply(classes, function(cn) {
        rows <- which(y == cn)
        colSums(X[rows, , drop = FALSE])
      })
      class_counts <- as.matrix(class_counts)   # rows=vocab, cols=classes
      rownames(class_counts) <- vocab
      colnames(class_counts) <- classes

      # Laplace smoothing
      totals <- colSums(class_counts)
      lik <- sweep(class_counts + 1, 2, totals + V, "/")

      # Store log-likelihoods (rows=classes, cols=words)
      log_lik <<- t(log(lik))

      invisible(TRUE)
    },

    predict = function(message) {
      # message: named numeric vector with word counts (bag-of-words)
      scores <- sapply(1:length(classes), function(i) {
        log_pr[i] + sum(message * log_lik[i, ], na.rm = TRUE)
      })
      names(scores) <- classes
      names(which.max(scores))  # return predicted class
    },

    score = function(X_test, y_test) {
      # Predict all test docs
      preds <- apply(X_test, 1, function(row) .self$predict(row))
      acc <- mean(preds == y_test)
      cm <- table(True = y_test, Pred = preds)
      metrics <- calculate_multiclass_metrics(cm, classes)

      list(
        accuracy = acc,
        confusion_matrix = cm,
        metrics_per_class = metrics$per_class,
        macro_avg_metrics = metrics$macro_avg
      )
    }
  )
)

# Example usage:
# model = naiveBayes()
# model$fit(X_train, y_train)
# model$predict(X_train[1, ])
# model$score(X_test, y_test)
```

## Measure effectiveness of your classifier

-   Note that accuracy is not always a good metric for your classifier.
    Look at precision and recall curves, F1 score metric.

    When evaluating the model, it's important to understand the
    different types of classification results:

    -   A ***true positive*** result is one where the model correctly
        predicts the positive class.
    -   A ***true negative*** result is one where the model correctly
        predicts the negative class.
    -   A ***false positive*** result is one where the model incorrectly
        predicts the positive class when it is actually negative.
    -   A ***false negative*** result is one where the model incorrectly
        predicts the negative class when it is actually positive.

    Precision measures the proportion of true positive predictions among
    all positive predictions made by the model.

    $$
    Precision = \frac{TP}{TP+FP}
    $$

    Recall, on the other hand, measures the proportion of true positives
    identified out of all actual positive cases.

    $$
    Recall = \frac{TP}{TP+FN}
    $$

    F1 score is the harmonic mean of both precision and recall.

    $$
    F1 = \frac{2\times Precision \times Recall}{Precision + Recall}
    $$

    **See [this
    link](https://cohere.com/blog/classification-eval-metrics) to find
    more information about metrics.**
```{r}
calculate_multiclass_metrics <- function(cm, class_labels) {
  num_classes <- length(class_labels)

  # Initialize vectors to store per-class metrics
  precision_per_class <- numeric(num_classes)
  recall_per_class    <- numeric(num_classes)
  f1_per_class        <- numeric(num_classes)

  names(precision_per_class) <- class_labels
  names(recall_per_class)    <- class_labels
  names(f1_per_class)        <- class_labels

  for (i in seq_along(class_labels)) {
    # For a given class, consider it as the "positive" class
    current_class <- class_labels[i]

    TP <- cm[current_class, current_class]
    FP <- sum(cm[, current_class]) - TP
    FN <- sum(cm[current_class, ]) - TP
    TN <- sum(cm) - TP - FP - FN # Total - TP - FP - FN

    # Precision: TP / (TP + FP)
    precision <- ifelse((TP + FP) > 0, TP / (TP + FP), 0)
    # Recall: TP / (TP + FN)
    recall <- ifelse((TP + FN) > 0, TP / (TP + FN), 0)
    # F1 Score: 2 * (Precision * Recall) / (Precision + Recall)
    f1 <- ifelse((precision + recall) > 0, 2 * (precision * recall) / (precision + recall), 0)

    precision_per_class[current_class] <- precision
    recall_per_class[current_class]    <- recall
    f1_per_class[current_class]        <- f1
  }

  # Macro-average for overall metrics
  macro_precision <- mean(precision_per_class)
  macro_recall    <- mean(recall_per_class)
  macro_f1        <- mean(f1_per_class)

  list(
    per_class = data.frame(
      class = class_labels,
      precision = precision_per_class,
      recall = recall_per_class,
      f1_score = f1_per_class
    ),
    macro_avg = data.frame(
      metric = c("Precision", "Recall", "F1_Score"),
      value = c(macro_precision, macro_recall, macro_f1)
    )
  )
}


# Create a mapping from document ID to author for both train and test sets
train_labels <- train %>% distinct(id, author)
test_labels <- test %>% distinct(id, author)

# Create DTM for the training set using tidytext
dtm_train <- tidy_text_train %>%
  count(id, splitted) %>%
  cast_dtm(document = id, term = splitted, value = n)

dtm_test_original <- tidy_text_test %>%
  count(id, splitted) %>%
  cast_dtm(document = id, term = splitted, value = n)


train_vocab <- colnames(dtm_train)
dtm_test_aligned <- matrix(0,
                           nrow = nrow(dtm_test_original),
                           ncol = length(train_vocab),
                           dimnames = list(rownames(dtm_test_original), train_vocab))

common_vocab <- intersect(colnames(dtm_test_original), train_vocab)

# Fill the new aligned matrix with the counts from the original test matrix.
# We convert the subset of the original DTM to a standard matrix before assigning.
dtm_test_aligned[, common_vocab] <- as.matrix(dtm_test_original[, common_vocab])

X_train <- as.matrix(dtm_train)
X_test <- dtm_test_aligned

# Create the label vectors y_train and y_test
# Ensure the order of labels matches the order of rows in the matrices
y_train <- train_labels$author[match(rownames(X_train), train_labels$id)]
y_test <- test_labels$author[match(rownames(X_test), test_labels$id)]

# Clean up intermediate objects to save memory
rm(dtm_train, dtm_test_original, dtm_test_aligned)

# Instantiate and fit the model using the training data
model <- naiveBayes()
model$fit(X_train, y_train)

# Get the performance results on the test data
results <- model$score(X_test, y_test)

# Print all the calculated metrics in a structured way
cat("====================================================\n")
cat("      CLASSIFIER PERFORMANCE EVALUATION\n")
cat("====================================================\n\n")

# Print Overall Accuracy
cat(sprintf("Overall Accuracy: %.4f (%.2f%%)\n\n", results$accuracy, results$accuracy * 100))
cat("----------------------------------------------------\n")

# Print Confusion Matrix
cat("Confusion Matrix:\n")
print(results$confusion_matrix)
cat("\n----------------------------------------------------\n")

# Print Per-Class Metrics
cat("Metrics per Class (Precision, Recall, F1-Score):\n")
print(results$metrics_per_class, row.names = FALSE)
cat("\n----------------------------------------------------\n")

# Print Macro-Averaged Metrics
cat("Macro-Averaged Metrics:\n")
print(results$macro_avg_metrics, row.names = FALSE)
cat("\n====================================================\n\n")


# a) Visualize the Confusion Matrix as a Heatmap for better interpretation
cm_df <- as.data.frame(results$confusion_matrix)
ggplot(cm_df, aes(x = Pred, y = True, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1, size = 6) +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Confusion Matrix Heatmap",
       x = "Predicted Author",
       y = "True Author") +
  theme_minimal() +
  theme(legend.position = "none")


# b) Visualize Per-Class F1 Scores for easy comparison
ggplot(results$metrics_per_class, aes(x = reorder(class, -f1_score), y = f1_score, fill = class)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_text(aes(label = sprintf("%.3f", f1_score)), vjust = -0.5) +
  ylim(0, 1) +
  labs(title = "F1 Score per Author",
       x = "Author",
       y = "F1 Score") +
  theme_minimal()


# Get predictions for the entire test set
predictions <- apply(X_test, 1, function(row) model$predict(row))

# Combine results into a single data frame
failure_df <- data.frame(
  id = rownames(X_test),
  true_author = y_test,
  predicted_author = predictions,
  stringsAsFactors = FALSE
)

# Find the cases where the model made a mistake
mistakes <- failure_df %>%
  filter(true_author != predicted_author) %>%
  # Join with the original test data to get the text
  left_join(test, by = "id") %>%
  select(id, text, true_author, predicted_author)

cat("\n====================================================\n")
cat("             ANALYSIS OF FAILURE CASES\n")
cat("====================================================\n\n")
cat(sprintf("The classifier made %d mistakes out of %d predictions.\n", nrow(mistakes), length(y_test)))
cat("Here are a few examples of incorrect classifications:\n\n")

# Display the first 5 failure cases (or fewer if there are not that many)
if (nrow(mistakes) > 0) {
  for (i in 1:min(5, nrow(mistakes))) {
    cat(sprintf("--- Example %d ---\n", i))
    cat(sprintf("Text: \"%s...\"\n", substr(mistakes$text[i], 1, 100)))
    cat(sprintf("True Author:      %s\n", mistakes$true_author[i]))
    cat(sprintf("Predicted Author: %s\n\n", mistakes$predicted_author[i]))
  }
} else {
  cat("Wow! There were no mistakes on the test set. Perfect classification!\n")
}

```
-   Visualize them.

-   Show failure cases.

Conclusions

Summarize your work by explaining in a few sentences the points listed
below.

-   Describe the method implemented in general. Show what are
    mathematical foundations you are basing your solution on.
-   List pros and cons of the method. This should include the
    limitations of your method, all the assumption you make about the
    nature of your data etc.
-   Explain why accuracy is not a good choice for the base metrics for
    classification tasks. Why F1 score is always preferable?

---
## Conclusions

In this lab, we successfully implemented a **Naive Bayes classifier** from
scratch in R to attribute text excerpts to one of three authors: Edgar
Allan Poe, Mary Shelley, or H.P. Lovecraft. The final model achieved a
strong overall **accuracy of 82.7%** and a macro-averaged **F1 score of
0.83**, demonstrating that the method is highly effective for this text
classification task based on word frequencies.

**Method and Mathematical Foundations**

The core of this project was the implementation of a **Multinomial Naive
Bayes classifier**, a probabilistic algorithm well-suited for text
classification. The model is founded on **Bayes' Theorem**:

$$
\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}
$$

This formula allows us to calculate the probability of an author (`class`)
given a piece of text (`observation`). The method's key assumption—and what
makes it "naive"—is that every word (`feature`) in the text is
**conditionally independent** of every other word, given the author. This
simplifies the likelihood calculation to a product of individual word
probabilities, $\prod \mathsf{P}(\mathrm{word}_i \mid \mathrm{class})$.
Probabilities were estimated from the training data, and we applied
**Laplace (add-one) smoothing** to handle words in the test set that were
not seen during training.

**Pros and Cons of the Method**

The Naive Bayes method has several distinct advantages and limitations.

-   **Pros:** The classifier is computationally efficient and fast, requiring
    a relatively small training set to estimate its parameters. Its
    underlying logic is simple to understand and implement. Despite its
    simplistic assumptions, it often performs surprisingly well, especially
    for text-based tasks.

-   **Cons:** The primary limitation is the **independence assumption**, as
    words in a language are rarely independent. This can lead to suboptimal
    results if features are strongly correlated. Another issue is the
    **zero-frequency problem**, where a word in the test set has not appeared
    in the training data for a class. This was mitigated in our
    implementation by using Laplace smoothing.

**Evaluation Metrics: Accuracy vs. F1 Score**

While **accuracy** is an intuitive metric, it can be misleading for
classification tasks, especially with an imbalanced class distribution. A
model could achieve high accuracy by simply predicting the majority class
every time, rendering it useless for identifying minority classes.

The **F1 score** is a more robust metric because it balances **precision** and
**recall** by calculating their harmonic mean.

-   **Precision** measures the proportion of true positive predictions
    among all positive predictions made (`TP / (TP + FP)`). It quantifies
    the quality of positive predictions.

-   **Recall** measures the proportion of true positives that were
    correctly identified out of all actual positives (`TP / (TP + FN)`). It
    quantifies the completeness of positive predictions.

By combining these two perspectives, the F1 score provides a more
comprehensive assessment of a classifier's effectiveness. In our case, the
per-class F1 scores gave a clearer picture of the model's performance than
accuracy alone, showing it was most effective at identifying H.P. Lovecraft
(F1 = 0.89) and struggled most with Edgar Allan Poe (F1 = 0.76).